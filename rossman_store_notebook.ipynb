{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alpha-polymer",
   "metadata": {},
   "source": [
    "## Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost graphviz lightgbm scikit-learn xgboost lightgbm seaborn numpy plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d27eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-paintball",
   "metadata": {},
   "source": [
    "Let's load the data into Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fewer-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "ross_df = pd.read_csv('train.csv')\n",
    "store_df = pd.read_csv('store.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "submission_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sophisticated-belle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version https://git-lfs.github.com/spec/v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oid sha256:f6e4597c142d7d909a13d53b68a8e85c00b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>size 38057952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          version https://git-lfs.github.com/spec/v1\n",
       "0  oid sha256:f6e4597c142d7d909a13d53b68a8e85c00b...\n",
       "1                                      size 38057952"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ross_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "favorite-recovery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version https://git-lfs.github.com/spec/v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oid sha256:e75f79972de046d88c2fd55da19df627f5c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>size 1427425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          version https://git-lfs.github.com/spec/v1\n",
       "0  oid sha256:e75f79972de046d88c2fd55da19df627f5c...\n",
       "1                                       size 1427425"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-conditions",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Read the data description provided on the competition page to understand what the values in each column of `store_df` represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-mambo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "obvious-immune",
   "metadata": {},
   "source": [
    "Let's merge the information from `store_df` into `train_df` and `test_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = ross_df.merge(store_df, how='left', on='Store')\n",
    "merged_test_df = test_df.merge(store_df, how='left', on='Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-incident",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Perform exploratory data analysis and visualization on the dataset. Study the distribution of values in each column, and their relationship with the target column `Sales`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-variable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-pastor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "confidential-identity",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-lying",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Engineering\n",
    "\n",
    "Let's take a look at the available columns, and figure out if we can create new columns or apply any useful transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-vermont",
   "metadata": {},
   "source": [
    "\n",
    "### Date\n",
    "\n",
    "First, let's convert `Date` to a `datetime` column and extract different parts of the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_date(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Year'] = df.Date.dt.year\n",
    "    df['Month'] = df.Date.dt.month\n",
    "    df['Day'] = df.Date.dt.day\n",
    "    df['WeekOfYear'] = df.Date.dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date(merged_df)\n",
    "split_date(merged_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-behavior",
   "metadata": {},
   "source": [
    "### Store Open/Closed\n",
    "\n",
    "Next, notice that the sales are zero whenever the store is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df.Open == 0].Sales.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-mount",
   "metadata": {},
   "source": [
    "Instead of trying to model this relationship, it would be better to hard-code it in our predictions, and remove the rows where the store is closed. We won't remove any rows from the test set, since we need to make predictions for every row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df.Open == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.histplot(data=merged_df, x='Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19861358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#plt.figure(figsize=(15, 8))\n",
    "temp_df = merged_df.sample(40000)\n",
    "sns.scatterplot(x=temp_df.Sales, y=temp_df.Customers, hue=temp_df.Date.dt.year, alpha=0.8)\n",
    "plt.title('sales vs customers')\"\"\"\n",
    "\n",
    "temp_df = merged_df.sample(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0047fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df972c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#plt.figure(figsize=(18,8))\n",
    "sns.scatterplot(x=temp_df.Store, y=temp_df.Sales, hue=temp_df.Date.dt.year, alpha=0.8)\n",
    "plt.title('stores vs sales')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fdc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc4389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ecc682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d530ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.corr()['Sales'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-giving",
   "metadata": {},
   "source": [
    "### Competition\n",
    "\n",
    "Next, we can use the columns `CompetitionOpenSince[Month/Year]` columns from `store_df` to compute the number of months for which a competitor has been open near the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_months(df):\n",
    "    df['CompetitionOpen'] = 12 * (df.Year - df.CompetitionOpenSinceYear) + (df.Month - df.CompetitionOpenSinceMonth)\n",
    "    df['CompetitionOpen'] = df['CompetitionOpen'].map(lambda x: 0 if x < 0 else x).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_months(merged_df)\n",
    "comp_months(merged_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-nutrition",
   "metadata": {},
   "source": [
    "Let's view the results of the new columns we've created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-fireplace",
   "metadata": {},
   "source": [
    "### Additional Promotion\n",
    "\n",
    "We can also add some additional columns to indicate how long a store has been running `Promo2` and whether a new round of `Promo2` starts in the current month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_promo_month(row):\n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun',              \n",
    "                 7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    try:\n",
    "        months = (row['PromoInterval'] or '').split(',')\n",
    "        if row['Promo2Open'] and month2str[row['Month']] in months:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def promo_cols(df):\n",
    "    # Months since Promo2 was open\n",
    "    df['Promo2Open'] = 12 * (df.Year - df.Promo2SinceYear) +  (df.WeekOfYear - df.Promo2SinceWeek)*7/30.5\n",
    "    df['Promo2Open'] = df['Promo2Open'].map(lambda x: 0 if x < 0 else x).fillna(0) * df['Promo2']\n",
    "    # Whether a new round of promotions was started in the current month\n",
    "    df['IsPromo2Month'] = df.apply(check_promo_month, axis=1) * df['Promo2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11cd8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_cols(merged_df)\n",
    "promo_cols(merged_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-surfing",
   "metadata": {},
   "source": [
    "Let's view the results of the columns we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[['Date', 'Promo2', 'Promo2SinceYear', 'Promo2SinceWeek', 'PromoInterval', 'Promo2Open', 'IsPromo2Month']].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-politics",
   "metadata": {},
   "source": [
    "The features related to competition and promotion are now much more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-property",
   "metadata": {},
   "source": [
    "### Input and Target Columns\n",
    "\n",
    "Let's select the columns that we'll use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday' ,'Promo2',\n",
    "              'StoreType', 'Assortment', 'CompetitionDistance','CompetitionOpen',\n",
    "              'Day', 'Month', 'Year', 'WeekOfYear', 'Promo2Open', 'IsPromo2Month']\n",
    "target_col = 'Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = merged_df[input_cols].copy()\n",
    "targets = merged_df[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = merged_test_df[input_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['Store', 'Promo', 'SchoolHoliday' ,'Promo2',\n",
    "             'CompetitionDistance','CompetitionOpen',\n",
    "              'Day', 'Month', 'Year', 'WeekOfYear', 'Promo2Open', 'IsPromo2Month']\n",
    "categorical_cols = ['DayOfWeek', 'StateHoliday', 'StoreType', 'Assortment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-wrapping",
   "metadata": {},
   "source": [
    "### Impute missing numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[numeric_cols].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482dd3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distance = inputs.CompetitionDistance.max()\n",
    "max_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d906383",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['CompetitionDistance'].fillna(max_distance*2, inplace=True)\n",
    "test_inputs['CompetitionDistance'].fillna(max_distance*2, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean').fit(train_inputs[numeric_cols])\n",
    "train_inputs[numeric_cols] = imputer.transform(train_inputs[numeric_cols])\n",
    "val_inputs[numeric_cols] = imputer.transform(val_inputs[numeric_cols])\n",
    "test_inputs[numeric_cols] = imputer.transform(test_inputs[numeric_cols])\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-commitment",
   "metadata": {},
   "source": [
    "Seems like competition distance is the only missing value, and we can simply fill it with the highest value (to indicate that competition is very far away)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-percentage",
   "metadata": {},
   "source": [
    "### Scale Numeric Values\n",
    "\n",
    "Let's scale numeric values to the 0 to 1 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(inputs[numeric_cols])\n",
    "inputs[numeric_cols] = scaler.transform(inputs[numeric_cols])\n",
    "test_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-thompson",
   "metadata": {},
   "source": [
    "### Encode Categorical Columns\n",
    "\n",
    "<img src=\"https://i.imgur.com/n8GuiOO.png\" width=\"640\">\n",
    "\n",
    "Let's one-hot encode categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore').fit(inputs[categorical_cols])\n",
    "encoded_cols = list(encoder.get_feature_names_out(categorical_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[encoded_cols] = encoder.transform(inputs[categorical_cols])\n",
    "\n",
    "test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-mailman",
   "metadata": {},
   "source": [
    "Finally, let's extract out all the numeric data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = inputs[numeric_cols + encoded_cols]\n",
    "X_test = test_inputs[numeric_cols + encoded_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-conversation",
   "metadata": {},
   "source": [
    "We haven't created a validation set yet, because we'll use K-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-apparel",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Look through the notebooks created by participants in the Kaggle competition and apply some other ideas for feature engineering. https://www.kaggle.com/c/rossmann-store-sales/code?competitionId=4594&sortBy=voteCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1b5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-energy",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-casting",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "We're now ready to train our gradient boosting machine (GBM) model. Here's how a GBM model works:\n",
    "\n",
    "1. The average value of the target column and uses as an initial prediction every input.\n",
    "2. The residuals (difference) of the predictions with the targets are computed.\n",
    "3. A decision tree of limited depth is trained to **predict just the residuals** for each input.\n",
    "4. Predictions from the decision tree are scaled using a parameter called the learning rate (this prevents overfitting)\n",
    "5. Scaled predictions for the tree are added to the previous predictions to obtain the new and improved predictions.\n",
    "6. Steps 2 to 5 are repeated to create new decision trees, each of which is trained to predict just the residuals from the previous prediction.\n",
    "\n",
    "The term \"gradient\" refers to the fact that each decision tree is trained with the purpose of reducing the loss from the previous iteration (similar to gradient descent). The term \"boosting\" refers the general technique of training new models to improve the results of an existing model. \n",
    "\n",
    "> **EXERCISE**: Can you describe in your own words how a gradient boosting machine is different from a random forest?\n",
    "\n",
    "\n",
    "For a mathematical explanation of gradient boosting, check out the following resources:\n",
    "\n",
    "- [XGBoost Documentation](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)\n",
    "- [Video Tutorials on StatQuest](https://www.youtube.com/watch?v=3CC4N4z3GJc&list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6)\n",
    "\n",
    "Here's a visual representation of gradient boosting:\n",
    "\n",
    "![](https://miro.medium.com/max/560/1*85QHtH-49U7ozPpmA5cAaw.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-mentor",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To train a GBM, we can use the `XGBRegressor` class from the [`XGBoost`](https://xgboost.readthedocs.io/en/latest/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "?XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(random_state=42, n_jobs=-1, n_estimators=20, max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-toyota",
   "metadata": {},
   "source": [
    "Let's train the model using `model.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-billion",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Explain how the `.fit` method of `XGBRegressor` applies the iterative machine learning workflow to train the model using the training data.\n",
    "> \n",
    "> <img src=\"https://www.deepnetts.com/blog/wp-content/uploads/2019/02/SupervisedLearning.png\" width=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-mount",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "We can now make predictions and evaluate the model using `model.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-pixel",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Let's evaluate the predictions using RMSE error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def rmse(a, b):\n",
    "    # For older scikit-learn versions (< 0.22.0)\n",
    "    try:\n",
    "        # Try with squared parameter (newer versions)\n",
    "        return mean_squared_error(a, b, squared=False)\n",
    "    except TypeError:\n",
    "        # Fall back to manual sqrt for older versions\n",
    "        return np.sqrt(mean_squared_error(a, b))\n",
    "\n",
    "rmse(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf20a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-premiere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "split-forty",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "We can visualize individual trees using `plot_tree` (note: this requires the `graphviz` library to be installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_tree\n",
    "from matplotlib.pylab import rcParams\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams['figure.figsize'] = 30,30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(model, rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(model, rankdir='LR', num_trees=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(model, rankdir='LR', num_trees=19);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-arthur",
   "metadata": {},
   "source": [
    "Notice how the trees only compute residuals, and not the actual target value. We can also visualize the tree as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = model.get_booster().get_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trees[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-stupid",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "Just like decision trees and random forests, XGBoost also provides a feature importance score for each column in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-element",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Feature Importance')\n",
    "sns.barplot(data=importance_df.head(10), x='importance', y='feature');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-diameter",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-intro",
   "metadata": {},
   "source": [
    "## K Fold Cross Validation\n",
    "\n",
    "Notice that we didn't create a validation set before training our XGBoost model. We'll use a different validation strategy this time, called K-fold cross validation ([source](https://vitalflux.com/k-fold-cross-validation-python-example/)):\n",
    "\n",
    "![](https://vitalflux.com/wp-content/uploads/2020/08/Screenshot-2020-08-15-at-11.13.53-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-amsterdam",
   "metadata": {},
   "source": [
    "Scikit-learn provides utilities for performing K fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-bulletin",
   "metadata": {},
   "source": [
    "Let's define a helper function `train_and_evaluate` which trains a model the given parameters and returns the trained model, training error and validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, train_targets, X_val, val_targets, **params):\n",
    "    model = XGBRegressor(random_state=42, n_jobs=-1, **params)\n",
    "    model.fit(X_train, train_targets)\n",
    "    train_rmse = rmse(model.predict(X_train), train_targets)\n",
    "    val_rmse = rmse(model.predict(X_val), val_targets)\n",
    "    return model, train_rmse, val_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-japanese",
   "metadata": {},
   "source": [
    "Now, we can use the `KFold` utility to create the different training/validations splits and train a separate model for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for train_idxs, val_idxs in kfold.split(X):\n",
    "    X_train, train_targets = X.iloc[train_idxs], targets.iloc[train_idxs]\n",
    "    X_val, val_targets = X.iloc[val_idxs], targets.iloc[val_idxs]\n",
    "    model, train_rmse, val_rmse = train_and_evaluate(X_train, \n",
    "                                                     train_targets, \n",
    "                                                     X_val, \n",
    "                                                     val_targets, \n",
    "                                                     max_depth=4, \n",
    "                                                     n_estimators=20)\n",
    "    models.append(model)\n",
    "    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-connecticut",
   "metadata": {},
   "source": [
    "Let's also define a function to average predictions from the 5 different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_avg(models, inputs):\n",
    "    return np.mean([model.predict(inputs) for model in models], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict_avg(models, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-meter",
   "metadata": {},
   "source": [
    "We can now use `predict_avg` to make predictions for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-population",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning and Regularization\n",
    "\n",
    "Just like other machine learning models, there are several hyperparameters we can to adjust the capacity of model and reduce overfitting.\n",
    "\n",
    "<img src=\"https://i.imgur.com/EJCrSZw.png\" width=\"480\">\n",
    "\n",
    "Check out the following resources to learn more about hyperparameter supported by XGBoost:\n",
    "\n",
    "- https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
    "- https://xgboost.readthedocs.io/en/latest/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "?XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-grave",
   "metadata": {},
   "source": [
    "Here's a helper function to test hyperparameters with K-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_params_kfold(n_splits, **params):\n",
    "    train_rmses, val_rmses, models = [], [], []\n",
    "    kfold = KFold(n_splits)\n",
    "    for train_idxs, val_idxs in kfold.split(X):\n",
    "        X_train, train_targets = X.iloc[train_idxs], targets.iloc[train_idxs]\n",
    "        X_val, val_targets = X.iloc[val_idxs], targets.iloc[val_idxs]\n",
    "        model, train_rmse, val_rmse = train_and_evaluate(X_train, train_targets, X_val, val_targets, **params)\n",
    "        models.append(model)\n",
    "        train_rmses.append(train_rmse)\n",
    "        val_rmses.append(val_rmse)\n",
    "    print('Train RMSE: {}, Validation RMSE: {}'.format(np.mean(train_rmses), np.mean(val_rmses)))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-america",
   "metadata": {},
   "source": [
    "Since it may take a long time to perform 5-fold cross validation for each set of parameters we wish to try, we'll just pick a random 10% sample of the dataset as the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, train_targets, val_targets = train_test_split(X, targets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_params(**params):\n",
    "    model = XGBRegressor(n_jobs=-1, random_state=42, **params)\n",
    "    model.fit(X_train, train_targets)\n",
    "    train_rmse = rmse(model.predict(X_train), train_targets)\n",
    "    val_rmse = rmse(model.predict(X_val), val_targets)\n",
    "    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-century",
   "metadata": {},
   "source": [
    "#### `n_estimators`\n",
    "\n",
    "The number of trees to be created. More trees = greater capacity of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(n_estimators=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(n_estimators=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-shelf",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Experiment with different values of `n_estimators`, plot a graph of the training and validation error and determine the best value for `n_estimators`.\n",
    "> \n",
    "> <img src=\"https://i.imgur.com/EJCrSZw.png\" width=\"360\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-electron",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-kruger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "talented-tunisia",
   "metadata": {},
   "source": [
    "#### `max_depth`\n",
    "\n",
    "As you increase the max depth of each tree, the capacity of the tree increases and it can capture more information about the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-tongue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-rental",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "colonial-mentor",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Experiment with different values of `max_depth`, plot a graph of the training and validation error and determine the optimal.\n",
    "> \n",
    "> <img src=\"https://i.imgur.com/EJCrSZw.png\" width=\"360\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-integrity",
   "metadata": {},
   "source": [
    "#### `learning_rate`\n",
    "\n",
    "The scaling factor to be applied to the prediction of each tree. A very high learning rate (close to 1) will lead to overfitting, and a low learning rate (close to 0) will lead to underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(n_estimators=50, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(n_estimators=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(n_estimators=50, learning_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(n_estimators=50, learning_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(n_estimators=50, learning_rate=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-louis",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Experiment with different values of `learning_rate`, plot a graph of the training and validation error and determine the optimal.\n",
    "> \n",
    "> <img src=\"https://i.imgur.com/EJCrSZw.png\" width=\"360\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-enemy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-procedure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "superior-paintball",
   "metadata": {},
   "source": [
    "#### `booster`\n",
    "\n",
    "Instead of using Decision Trees, XGBoost can also train a linear model for each iteration. This can be configured using `booster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(booster='gblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-distance",
   "metadata": {},
   "source": [
    "Clearly, a linear model is not well suited for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-philippines",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Exeperiment with other hyperparameters like `gamma`, `min_child_weight`, `max_delta_step`, `subsample`, `colsample_bytree` etc. and find their optimal values. Learn more about them here: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-currency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-young",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "steady-sociology",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Train a model with your best hyperparmeters and evaluate its peformance using 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-associate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-consistency",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-radar",
   "metadata": {},
   "source": [
    "## Putting it Together and Making Predictions\n",
    "\n",
    "Let's train a final model on the entire training set with custom hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(n_jobs=-1, random_state=42, n_estimators=1000, \n",
    "                     learning_rate=0.2, max_depth=10, subsample=0.9, \n",
    "                     colsample_bytree=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-religious",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-garbage",
   "metadata": {},
   "source": [
    "Let's add the predictions into `submission_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['Sales']  = test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-football",
   "metadata": {},
   "source": [
    "Recall, however, if if the store is not open, then the sales must be 0. Thus, wherever the value of `Open` in the test set is 0, we can set the sales to 0. Also, there some missing values for `Open` in the test set. We'll replace them with 1 (open)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.Open.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['Sales'] = submission_df['Sales'] * test_df.Open.fillna(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-characteristic",
   "metadata": {},
   "source": [
    "We can now save the predictions as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't work on Colab, use the file browser instead.\n",
    "FileLink('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-right",
   "metadata": {},
   "source": [
    "We can now make a submission on this page and check our score: https://www.kaggle.com/c/rossmann-store-sales/submit\n",
    "\n",
    "![](https://i.imgur.com/bQ0lpSJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-affiliation",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Experiment with different models and hyperparameters and try to beat the above score. Take inspiration from the [top notebooks](https://www.kaggle.com/c/rossmann-store-sales/code?competitionId=4594&sortBy=voteCount) on the \"Code\" tab of the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-terminology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-detail",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "affected-frost",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Save the model and all the other required objects using `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-compact",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-barrel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "answering-nelson",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Write a function `predict_input` which can make predictions for a single input provided as a dictionary. Make sure to include all the feature engineering and preprocessing steps. Refer to previous tutorials for hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-picnic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-cattle",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-upset",
   "metadata": {},
   "source": [
    "## Summary and References\n",
    "\n",
    "![](https://miro.medium.com/max/560/1*85QHtH-49U7ozPpmA5cAaw.png)\n",
    "\n",
    "The following topics were covered in this tutorial:\n",
    "\n",
    "- Downloading a real-world dataset from a Kaggle competition\n",
    "- Performing feature engineering and prepare the dataset for training\n",
    "- Training and interpreting a gradient boosting model using XGBoost\n",
    "- Training with KFold cross validation and ensembling results\n",
    "- Configuring the gradient boosting model and tuning hyperparamters\n",
    "\n",
    "Check out these resources to learn more: \n",
    "\n",
    "- https://albertum.medium.com/l1-l2-regularization-in-xgboost-regression-7b2db08a59e0\n",
    "- https://machinelearningmastery.com/evaluate-gradient-boosting-models-xgboost-python/\n",
    "- https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
    "- https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "- https://www.kaggle.com/xwxw2929/rossmann-sales-top1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8c8db",
   "metadata": {},
   "source": [
    "## Revision Questions\n",
    "1.\tWhat is a Gradient Boosting Machine (GBM) model?\n",
    "2.\tWhat does term gradient refer to?\n",
    "3.\tWhat does term boosting refer to?\n",
    "4.\tWhat are weights and bias?\n",
    "5.\tHow to choose differentiate/choose numerical and categorical columns?\n",
    "6.\tWhy do you scale numerical columns?\n",
    "7.\tWhy do you encode categorical columns?\n",
    "8.\tWhat is rankdir in <code>plot_tree()</code>?\n",
    "9.\tWhat is the working of K-fold cross validation?\n",
    "10.\tHow does gamma hyperparameter work?\n",
    "11.\tWhat is generalization?\n",
    "12.\tWhat is ensembling?\n",
    "13.\tWhat are the different ways to impute missing data?\n",
    "14.\tWhat are the advantages of XGBoost?\n",
    "15.\tWhat are the disadvantages of XGBoost?\n",
    "16.\tWhat are the data pre-processing steps for XGBoost?\n",
    "17.\tWhat is <code>rcParams</code>?\n",
    "18.\tWhat does <code>get_dump()</code> do?\n",
    "19.\tWhat is the difference between XGBoost and LightGBM?\n",
    "20.\tIs XGBoost faster than Random Forest? If so, why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
